{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from pickle import TRUE\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from six.moves import cPickle\n",
    "import opts\n",
    "import models\n",
    "from dataloader import *\n",
    "from dataloaderraw import *\n",
    "import eval_utils\n",
    "import argparse\n",
    "import misc.utils as utils\n",
    "import torch\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(checkpoint_dir):\n",
    "    # Input arguments and options\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', type=str, default='save/'+checkpoint_dir+'/model-best.pth',\n",
    "                    help='path to model to evaluate')\n",
    "    parser.add_argument('--cnn_model', type=str,  default='resnet101',\n",
    "                    help='resnet101, resnet152')\n",
    "    parser.add_argument('--infos_path', type=str, default='save/'+checkpoint_dir+'/infos_'+checkpoint_dir+'-best.pkl',\n",
    "                    help='path to infos to evaluate')\n",
    "    # Basic options\n",
    "    parser.add_argument('--batch_size', type=int, default=1,\n",
    "                    help='if > 0 then overrule, otherwise load from checkpoint.')\n",
    "    parser.add_argument('--num_images', type=int, default=-1,\n",
    "                    help='how many images to use when periodically evaluating the loss? (-1 = all)')\n",
    "    parser.add_argument('--language_eval', type=int, default=1,\n",
    "                    help='Evaluate language as well (1 = yes, 0 = no)? BLEU/CIDEr/METEOR/ROUGE_L? requires coco-caption code from Github.')\n",
    "    parser.add_argument('--dump_images', type=int, default=0,\n",
    "                    help='Dump images into vis/imgs folder for vis? (1=yes,0=no)')\n",
    "    parser.add_argument('--dump_json', type=int, default=0,\n",
    "                    help='Dump json with predictions into vis folder? (1=yes,0=no)')\n",
    "    parser.add_argument('--dump_path', type=int, default=0,\n",
    "                    help='Write image paths along with predictions into vis json? (1=yes,0=no)')\n",
    "\n",
    "    # Sampling options\n",
    "    parser.add_argument('--sample_max', type=int, default=1,\n",
    "                    help='1 = sample argmax words. 0 = sample from distributions.')\n",
    "    ##########################################################\n",
    "    parser.add_argument('--beam_size', type=int, default=1,\n",
    "                    help='used when sample_max = 1, indicates number of beams in beam search. Usually 2 or 3 works well. More is not better. Set this to 1 for faster runtime but a bit worse performance.')\n",
    "    ##########################################################\n",
    "    parser.add_argument('--max_length', type=int, default=20,\n",
    "                    help='Maximum length during sampling')\n",
    "    parser.add_argument('--length_penalty', type=str, default='',\n",
    "                    help='wu_X or avg_X, X is the alpha')\n",
    "    parser.add_argument('--group_size', type=int, default=1,\n",
    "                    help='used for diverse beam search. if group_size is 1, then it\\'s normal beam search')\n",
    "    parser.add_argument('--diversity_lambda', type=float, default=0.5,\n",
    "                    help='used for diverse beam search. Usually from 0.2 to 0.8. Higher value of lambda produces a more diverse list')\n",
    "    parser.add_argument('--temperature', type=float, default=1.0,\n",
    "                    help='temperature when sampling from distributions (i.e. when sample_max = 0). Lower = \"safer\" predictions.')\n",
    "    parser.add_argument('--decoding_constraint', type=int, default=0,\n",
    "                    help='If 1, not allowing same word in a row')\n",
    "    parser.add_argument('--block_trigrams', type=int, default=0,\n",
    "                    help='block repeated trigram.')\n",
    "    parser.add_argument('--remove_bad_endings', type=int, default=0,\n",
    "                    help='Remove bad endings')\n",
    "    # For evaluation on a folder of images:\n",
    "    parser.add_argument('--image_folder', type=str, default='', \n",
    "                    help='If this is nonempty then will predict on the images in this folder path')\n",
    "    parser.add_argument('--image_root', type=str, default='', \n",
    "                    help='In case the image paths have to be preprended with a root path to an image folder')\n",
    "    # For evaluation on MSCOCO images from some split:\n",
    "    parser.add_argument('--input_fc_dir', type=str, default='',\n",
    "                    help='path to the h5file containing the preprocessed dataset')\n",
    "    parser.add_argument('--input_att_dir', type=str, default='',\n",
    "                    help='path to the h5file containing the preprocessed dataset')\n",
    "    parser.add_argument('--input_box_dir', type=str, default='',\n",
    "                    help='path to the h5file containing the preprocessed dataset')\n",
    "    parser.add_argument('--input_label_h5', type=str, default='',\n",
    "                    help='path to the h5file containing the preprocessed dataset')\n",
    "    parser.add_argument('--input_json', type=str, default='', \n",
    "                    help='path to the json file containing additional info and vocab. empty = fetch from model checkpoint.')\n",
    "    parser.add_argument('--split', type=str, default='val', \n",
    "                    help='if running on MSCOCO images, which split to use: val|test|train')\n",
    "    parser.add_argument('--coco_json', type=str, default='', \n",
    "                    help='if nonempty then use this file in DataLoaderRaw (see docs there). Used only in MSCOCO test evaluation, where we have a specific json file of only test set images.')\n",
    "    # misc\n",
    "    #######################################################\n",
    "    parser.add_argument('--id', type=str, default='', \n",
    "                    help='an id identifying this run/job. used only if language_eval = 1 for appending to intermediate files')\n",
    "    #######################################################\n",
    "    parser.add_argument('--verbose_beam', type=int, default=1, \n",
    "                    help='if we need to print out all beam search beams.')\n",
    "    parser.add_argument('--verbose_loss', type=int, default=1, \n",
    "                    help='if we need to calculate loss.')\n",
    "\n",
    "    opt = parser.parse_args([])\n",
    "    # Load infos\n",
    "    with open(opt.infos_path,'rb') as f:\n",
    "        infos = utils.pickle_load(f)\n",
    "    # pdb.set_trace()\n",
    "    # override and collect parameters\n",
    "    if len(opt.input_fc_dir) == 0:\n",
    "        opt.input_fc_dir = infos['opt'].input_fc_dir\n",
    "        opt.input_att_dir = infos['opt'].input_att_dir\n",
    "        opt.input_box_dir = getattr(infos['opt'], 'input_box_dir', '')\n",
    "        opt.input_label_h5 = infos['opt'].input_label_h5\n",
    "    if len(opt.input_json) == 0:\n",
    "        opt.input_json = infos['opt'].input_json\n",
    "    if opt.batch_size == 0:\n",
    "        opt.batch_size = infos['opt'].batch_size\n",
    "    if len(opt.id) == 0:\n",
    "        opt.id = infos['opt'].id\n",
    "    ignore = [\"id\", \"batch_size\", \"beam_size\", \"start_from\", \"language_eval\", \"block_trigrams\"]\n",
    "\n",
    "    for k in vars(infos['opt']).keys():\n",
    "        if k not in ignore:\n",
    "            if k in vars(opt):\n",
    "                assert vars(opt)[k] == vars(infos['opt'])[k], k + ' option not consistent'\n",
    "            else:\n",
    "                vars(opt).update({k: vars(infos['opt'])[k]}) # copy over options from model\n",
    "\n",
    "    vocab = infos['vocab'] # ix -> word mapping\n",
    "    \n",
    "    if 'l2r' in checkpoint_dir: \n",
    "        opt.cbt = False\n",
    "        opt.r2l = False\n",
    "        opt.input_label_h5 = 'data/cocotalk_bw_label.h5'\n",
    "        \n",
    "    if 'r2l' in checkpoint_dir: \n",
    "        opt.cbt = False\n",
    "\n",
    "    # Setup the model\n",
    "    model = models.setup(opt)\n",
    "    model.load_state_dict(torch.load(opt.model))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[]\n",
    "opts_list = []\n",
    "checkpoint_dirs = ['transformer-l2r','transformer-r2l','transformer-cb','transformer-cb-VinVL-feat']\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    model, opt = setup_model(checkpoint_dir)\n",
    "    model_list.append(model)\n",
    "    opts_list.append(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9487\n",
      "DataLoader loading h5 file:  data/mscoco/cocobu_fc data/mscoco/cocobu_att data/mscoco/cocobu_box data/cocotalk_bw_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n",
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9487\n",
      "DataLoader loading h5 file:  data/mscoco/cocobu_fc data/mscoco/cocobu_att data/mscoco/cocobu_box data/cocotalk_bw_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n",
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9489\n",
      "DataLoader loading h5 file:  data/mscoco/cocobu_fc data/mscoco/cocobu_att data/mscoco/cocobu_box data/cocotalk_bw_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n",
      "DataLoader loading json file:  data/cocotalk.json\n",
      "vocab size is  9489\n",
      "DataLoader loading h5 file:  data/mscoco_VinVL/cocobu_fc data/mscoco_VinVL/cocobu_att data/mscoco_VinVL/cocobu_box data/cocotalk_bw_label.h5\n",
      "max sequence length in data is 16\n",
      "read 123287 image features\n",
      "assigned 113287 images to split train\n",
      "assigned 5000 images to split val\n",
      "assigned 5000 images to split test\n"
     ]
    }
   ],
   "source": [
    "# Create the Data Loader instance\n",
    "loader = [DataLoader(opt) for opt in opts_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_list = [vars(opt) for opt in opts_list]\n",
    "split =  'test'\n",
    "os.environ[\"REMOVE_BAD_ENDINGS\"] = str(0) # Use this nasty way to make other code clean since it's a global configuration\n",
    "\n",
    "\n",
    "[_.reset_iterator(split) for _ in loader]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_id:540806\n",
      "transformer-l2r:['a man riding a motorcycle down a road next to a bridge']\n",
      "Image_id:540806\n",
      "transformer-r2l:['a person is riding a motorcycle under a bridge']\n",
      "Image_id:540806\n",
      "transformer-cb:['two people riding a motorcycle down a road under a bridge']\n",
      "Image_id:540806\n",
      "transformer-cb-VinVL-feat:['a couple of people riding a motorcycle down a road']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_list)):\n",
    "\n",
    "    data = loader[i].get_batch(split)\n",
    "    print('Image_id:{}'.format(data['infos'][0]['id']))\n",
    "\n",
    "\n",
    "    # forward the model to also get generated samples for each image\n",
    "    # Only leave one feature for each image, in case duplicate sample\n",
    "    tmp = [data['fc_feats'][np.arange(loader[i].batch_size) * loader[i].seq_per_img], \n",
    "        data['att_feats'][np.arange(loader[i].batch_size) * loader[i].seq_per_img],\n",
    "        data['att_masks'][np.arange(loader[i].batch_size) * loader[i].seq_per_img] if data['att_masks'] is not None else None]\n",
    "    tmp = [_.cuda() if _ is not None else _ for _ in tmp]\n",
    "    fc_feats, att_feats, att_masks = tmp\n",
    "\n",
    "    model = model_list[i]\n",
    "    model_name = checkpoint_dirs[i]\n",
    "    # forward the model to also get generated samples for each image\n",
    "    with torch.no_grad():\n",
    "        if opt_list[i]['cbt']:\n",
    "            seq, seqLogprobs = model(fc_feats, att_feats, att_masks, opt=opt_list[i], mode='sample')\n",
    "        else:\n",
    "            seq = model(fc_feats, att_feats, att_masks, opt=opt_list[i], mode='sample')[0].data\n",
    "\n",
    "        if opt_list[i]['r2l']:\n",
    "            sents = utils.decode_sequence(loader[i].get_vocab(), seq)\n",
    "            sents = [' '.join(sent.split()[::-1]) for sent in sents]\n",
    "        elif opt_list[i]['cbt']:\n",
    "            sents = []\n",
    "            seq = seq.view(-1, seq.size(-1))\n",
    "            sents_tmp, length_idx = utils.decode_sequence(loader[i].get_vocab(), seq, True)\n",
    "            seqLogprobs_sum = torch.cumsum(seqLogprobs, dim=2)\n",
    "            length_idx = torch.from_numpy(length_idx.reshape(seqLogprobs.size(0), seqLogprobs.size(1), 1)).to(seqLogprobs_sum.device)\n",
    "            seqLogprobs_sum = seqLogprobs_sum.gather(2, length_idx)\n",
    "            seqLogprobs_lp = utils.length_average(length_idx+1, seqLogprobs_sum).squeeze(-1)\n",
    "            choice = torch.max(seqLogprobs_lp,dim =-1, keepdim=True)[1].cpu()\n",
    "            index = torch.arange(seqLogprobs_lp.numel()).view(seqLogprobs_lp.size())\n",
    "            index = index.gather(1,choice).view(-1).tolist()\n",
    "            for i in index:\n",
    "                if i%2 == 0:\n",
    "                    sents.append(sents_tmp[i])\n",
    "                else:\n",
    "                    sents.append(' '.join(sents_tmp[i].split()[::-1]))\n",
    "        else:\n",
    "            sents = utils.decode_sequence(loader[i].get_vocab(), seq)\n",
    "        print('{}:{}'.format(model_name, sents))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
